{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import collections\n",
    "from sklearn import metrics\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Embedding, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 3074 records.\n",
      "Test set has 1318 records.\n"
     ]
    }
   ],
   "source": [
    "dt = pd.read_csv('C:/Users/Owner/Documents/Sac State/CSC215_P2_Stock_Price.csv')\n",
    "dt['Close_y'] = dt['Close']\n",
    "split = int(0.7 * len(dt))\n",
    "df_train = dt[:split]\n",
    "df_test = dt[split:len(dt)]\n",
    "\n",
    "print(\"Training set has {} records.\".format(len(df_train)))\n",
    "print(\"Test set has {} records.\".format(len(df_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to ensure time-based relationships between observations, we split the data in a sequential way. The first 70% of observations are in the training set, with the remaining in our test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_train = df_train['Close']\n",
    "close_test = df_test['Close']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save a pre-normalized copy of the Close values. This will be used as our dependent variable in models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "        \n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "        \n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "normal_list = ['Open', 'High', 'Low', 'Volume', 'Close'];\n",
    "\n",
    "for element in normal_list:\n",
    "    encode_numeric_zscore(df_train, element)\n",
    "    encode_numeric_zscore(df_test, element)    \n",
    "\n",
    "\n",
    "\n",
    "params_train = df_train[['Open', 'High', 'Low', 'Volume', 'Close']].values.tolist()\n",
    "params_test = df_test[['Open', 'High', 'Low', 'Volume', 'Close',]].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we normalize the values, and drop the 'date' column from the dataframe. The way we did this caused a warning, however this does not impact our final program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (3066, 7, 5)\n",
      "Shape of x_test: (1310, 7, 5)\n",
      "Shape of y_train: (3066,)\n",
      "Shape of y_test: (1310,)\n"
     ]
    }
   ],
   "source": [
    "def to_sequences(seq_size, data):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(data)-SEQUENCE_SIZE-1):\n",
    "        #print(i)\n",
    "        window = data[i:(i+SEQUENCE_SIZE)]\n",
    "        after_window = data[i+SEQUENCE_SIZE]\n",
    "        window = [x for x in window]\n",
    "        #print(\"{} - {}\".format(window,after_window))\n",
    "        x.append(window)\n",
    "        y.append(after_window)\n",
    "        \n",
    "    return np.array(x),np.array(y)\n",
    "\n",
    "SEQUENCE_SIZE = 7\n",
    "x_train,y_train = to_sequences(SEQUENCE_SIZE,params_train)\n",
    "obs_train = close_train[SEQUENCE_SIZE:len(close_train)].values.tolist()\n",
    "obs_train.pop()\n",
    "obs_train = np.asarray(obs_train)\n",
    "\n",
    "x_test,y_test = to_sequences(SEQUENCE_SIZE,params_test)\n",
    "obs_test = close_test[SEQUENCE_SIZE:len(close_test)].values.tolist()\n",
    "obs_test.pop()\n",
    "obs_test = np.asarray(obs_test)\n",
    "\n",
    "print(\"Shape of x_train: {}\".format(x_train.shape))\n",
    "print(\"Shape of x_test: {}\".format(x_test.shape))\n",
    "print(\"Shape of y_train: {}\".format(obs_train.shape))\n",
    "print(\"Shape of y_test: {}\".format(obs_test.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we transform our data in sequence data. Now, each record is a sequence of 7 days of data, each with 5 relevant pieces of information on that day's market behavior. Additionally, there is one dependent variable for each of these 7x5 structures which holds the close price of the stock on the seventh day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train on 3066 samples, validate on 1310 samples\n",
      "Epoch 1/100\n",
      "3066/3066 - 13s - loss: 0.0795 - val_loss: 0.0209\n",
      "Epoch 2/100\n",
      "3066/3066 - 13s - loss: 0.0223 - val_loss: 0.0171\n",
      "Epoch 3/100\n",
      "3066/3066 - 13s - loss: 0.0177 - val_loss: 0.0135\n",
      "Epoch 4/100\n",
      "3066/3066 - 14s - loss: 0.0158 - val_loss: 0.0120\n",
      "Epoch 5/100\n",
      "3066/3066 - 15s - loss: 0.0139 - val_loss: 0.0108\n",
      "Epoch 6/100\n",
      "3066/3066 - 9s - loss: 0.0130 - val_loss: 0.0118\n",
      "Epoch 7/100\n",
      "3066/3066 - 9s - loss: 0.0113 - val_loss: 0.0129\n",
      "Epoch 8/100\n",
      "3066/3066 - 10s - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 9/100\n",
      "3066/3066 - 10s - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 10/100\n",
      "3066/3066 - 14s - loss: 0.0083 - val_loss: 0.0097\n",
      "Epoch 11/100\n",
      "3066/3066 - 12s - loss: 0.0076 - val_loss: 0.0165\n",
      "Epoch 12/100\n",
      "3066/3066 - 12s - loss: 0.0070 - val_loss: 0.0084\n",
      "Epoch 13/100\n",
      "3066/3066 - 9s - loss: 0.0062 - val_loss: 0.0075\n",
      "Epoch 14/100\n",
      "3066/3066 - 10s - loss: 0.0055 - val_loss: 0.0086\n",
      "Epoch 15/100\n",
      "3066/3066 - 7s - loss: 0.0048 - val_loss: 0.0106\n",
      "Epoch 16/100\n",
      "3066/3066 - 10s - loss: 0.0056 - val_loss: 0.0091\n",
      "Epoch 17/100\n",
      "3066/3066 - 8s - loss: 0.0048 - val_loss: 0.0109\n",
      "Epoch 00017: early stopping\n",
      "1\n",
      "Train on 3066 samples, validate on 1310 samples\n",
      "Epoch 1/100\n",
      "3066/3066 - 15s - loss: 0.0768 - val_loss: 0.0170\n",
      "Epoch 2/100\n",
      "3066/3066 - 13s - loss: 0.0222 - val_loss: 0.0154\n",
      "Epoch 3/100\n",
      "3066/3066 - 14s - loss: 0.0232 - val_loss: 0.0123\n",
      "Epoch 4/100\n",
      "3066/3066 - 16s - loss: 0.0163 - val_loss: 0.0109\n",
      "Epoch 5/100\n",
      "3066/3066 - 13s - loss: 0.0155 - val_loss: 0.0134\n",
      "Epoch 6/100\n",
      "3066/3066 - 10s - loss: 0.0141 - val_loss: 0.0098\n",
      "Epoch 7/100\n",
      "3066/3066 - 8s - loss: 0.0117 - val_loss: 0.0106\n",
      "Epoch 8/100\n",
      "3066/3066 - 10s - loss: 0.0097 - val_loss: 0.0139\n",
      "Epoch 9/100\n",
      "3066/3066 - 7s - loss: 0.0076 - val_loss: 0.0106\n",
      "Epoch 10/100\n",
      "3066/3066 - 12s - loss: 0.0069 - val_loss: 0.0154\n",
      "Epoch 11/100\n",
      "3066/3066 - 9s - loss: 0.0059 - val_loss: 0.0121\n",
      "Epoch 00011: early stopping\n",
      "Training finished...Loading the best model\n",
      "\n",
      "0\n",
      "Train on 3066 samples, validate on 1310 samples\n",
      "Epoch 1/100\n",
      "3066/3066 - 10s - loss: 0.0661 - val_loss: 0.0190\n",
      "Epoch 2/100\n",
      "3066/3066 - 6s - loss: 0.0220 - val_loss: 0.0147\n",
      "Epoch 3/100\n",
      "3066/3066 - 10s - loss: 0.0193 - val_loss: 0.0148\n",
      "Epoch 4/100\n",
      "3066/3066 - 13s - loss: 0.0158 - val_loss: 0.0126\n",
      "Epoch 5/100\n",
      "3066/3066 - 13s - loss: 0.0123 - val_loss: 0.0099\n",
      "Epoch 6/100\n",
      "3066/3066 - 11s - loss: 0.0119 - val_loss: 0.0091\n",
      "Epoch 7/100\n",
      "3066/3066 - 12s - loss: 0.0098 - val_loss: 0.0105\n",
      "Epoch 8/100\n",
      "3066/3066 - 14s - loss: 0.0080 - val_loss: 0.0097\n",
      "Epoch 9/100\n",
      "3066/3066 - 12s - loss: 0.0073 - val_loss: 0.0136\n",
      "Epoch 10/100\n",
      "3066/3066 - 12s - loss: 0.0060 - val_loss: 0.0093\n",
      "Epoch 00010: early stopping\n",
      "1\n",
      "Train on 3066 samples, validate on 1310 samples\n",
      "Epoch 1/100\n",
      "3066/3066 - 16s - loss: 0.0985 - val_loss: 0.0200\n",
      "Epoch 2/100\n",
      "3066/3066 - 16s - loss: 0.0239 - val_loss: 0.0142\n",
      "Epoch 3/100\n",
      "3066/3066 - 13s - loss: 0.0208 - val_loss: 0.0165\n",
      "Epoch 4/100\n",
      "3066/3066 - 12s - loss: 0.0167 - val_loss: 0.0088\n",
      "Epoch 5/100\n",
      "3066/3066 - 10s - loss: 0.0133 - val_loss: 0.0087\n",
      "Epoch 6/100\n",
      "3066/3066 - 10s - loss: 0.0134 - val_loss: 0.0107\n",
      "Epoch 7/100\n",
      "3066/3066 - 11s - loss: 0.0115 - val_loss: 0.0066\n",
      "Epoch 8/100\n",
      "3066/3066 - 7s - loss: 0.0103 - val_loss: 0.0095\n",
      "Epoch 9/100\n",
      "3066/3066 - 11s - loss: 0.0107 - val_loss: 0.0166\n",
      "Epoch 10/100\n",
      "3066/3066 - 8s - loss: 0.0079 - val_loss: 0.0125\n",
      "Epoch 11/100\n",
      "3066/3066 - 13s - loss: 0.0077 - val_loss: 0.0093\n",
      "Epoch 12/100\n",
      "3066/3066 - 10s - loss: 0.0061 - val_loss: 0.0125\n",
      "Epoch 00012: early stopping\n",
      "Training finished...Loading the best model\n",
      "\n",
      "Analyzing model with optimizer adam\n",
      "Score (RMSE): 20.40160763662155\n",
      "Analyzing model with optimizer sgd\n",
      "Score (RMSE): 20.40160763662155\n"
     ]
    }
   ],
   "source": [
    "myDict = dict()\n",
    "##activationType = ['relu', 'sigmoid', 'tanh']\n",
    "optimizerType = ['adam', 'sgd']\n",
    "iteration = 0\n",
    "\n",
    "for opt in optimizerType:\n",
    "    checkpointer = ModelCheckpoint(filepath=\"C:/Users/Owner/Documents/Sac State/csc215/proj2/best_weights.hdf5\", verbose=0, save_best_only=True) # save best model        \n",
    "    for i in range(2):\n",
    "        print(i)        \n",
    "        # Build network\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(64, dropout=0.1, recurrent_dropout=0.1, input_shape=(SEQUENCE_SIZE, 5)))\n",
    "        model.add(Dense(32))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')        \n",
    "        monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')        \n",
    "        model.fit(x_train,obs_train,validation_data=(x_test,obs_test), callbacks=[monitor],verbose=2, epochs=100)  \n",
    "\n",
    "\n",
    "    print('Training finished...Loading the best model')  \n",
    "    print()\n",
    "    model.load_weights(\"C:/Users/Owner/Documents/Sac State/csc215/proj2/best_weights.hdf5\") # load weights from best model\n",
    "    myDict.update({iteration : (opt, model)})\n",
    "    iteration += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for ele in myDict.values():\n",
    "    print('Analyzing model with optimizer {}'.format(ele[0]))\n",
    "    model = ele[1]\n",
    "    pred = model.predict(x_test)\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred,obs_test))\n",
    "    print(\"Score (RMSE): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
